{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print u'Так надо'\n",
    "\n",
    "df_auto = pd.read_csv('http://bit.ly/1gIQs6C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/header.png\"></center>\n",
    "\n",
    "<h1><center>Алгоритмы интеллектуальной обработки больших объемов данных</center></h1>\n",
    "<hr>\n",
    "<h2><center>Линейные модели</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quiz Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://imgs.xkcd.com/comics/linear_regression.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Линейные модели\n",
    "\n",
    "* Линейная регрессия\n",
    "* Логистическая регрессия\n",
    "* Метод опорных векторов (Support Vector Machine)\n",
    "\n",
    "\n",
    "* *Обобщенные линейные модели\n",
    "* *LDA (Linear Discriminant Analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_auto.plot(x='mileage', y='price', kind='scatter', s=120) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Постановка задачи\n",
    "\n",
    "* Дано описание $n$ объектов по $m$ признакам в виде матрицы размера $n \\times m$: $X = [x^{(i)}_j]^{i=1\\dots n}_{j=1\\dots m} $.<br\\> ($x^{(i)}_j$ означает $j$-ый признак $i$-го объекта) <br\\>\n",
    "* Дана **вещественная** зависимая переменная, которая тоже имеет отношение к этим объекам: $y \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Немного формул\n",
    "Наша задача, выявить **линейную** зависимость между признаками в $X$ и значениями в $y$:\n",
    "$$a(X) = \\hat{y} = X\\beta \\quad \\Leftrightarrow \\quad a(x^{(i)}) = \\hat{y}^{(i)} = \\beta_0 + \\beta_1x^{(i)}_1 + \\dots$$\n",
    "Необходимо оценить коэффициенты $\\beta_i$.\n",
    "\n",
    "В случае линейной регрессии коэффициенты $\\beta_i$ рассчитываются так, чтобы минимизировать сумму квадратов ошибок по всем наблюдениям:\n",
    "$$ L(\\beta) = \\frac{1}{2n}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2n}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min\\limits_{\\beta} $$ $$ \\Updownarrow $$  $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 + \\dots - y^{(i)})^2  \\rightarrow \\min\\limits_{\\beta_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Можно получить  решение аналитически\n",
    "\n",
    "Пусть $a(x) = \\beta_0 + \\beta_1x_1$\n",
    "\n",
    "Считаем частные производные по $\\beta_0$, $\\beta_1$, приравниваем к $0$, получаем нужные значения коэффициентов:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)}) = 0$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Можно получить  решение аналитически\n",
    "\n",
    "Можно обобщить на большее количество признаков и перевести в матричный вид:\n",
    "\n",
    "$$ X^\\top X\\beta - X^\\top y = 0 \\quad \\Leftrightarrow \\quad \\beta = (X^\\top X)^{-1} X^\\top y \\quad\\text{(Normal Equation)}$$\n",
    "\n",
    "* [Matrix Calculus](http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/)\n",
    "* [Matrix Cookbook](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Почему так делать не стоит?\n",
    "* Расчет обратной матрицы - дорогая и не совсем устойчивая операция (хотя бы потому что не у всякой матрицы есть обратная)\n",
    "* Лучше будет воспользоваться методами оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "Производная функции $f(x)$ в точке $f(x_0)$:\n",
    "$$ f'(x_0) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_0+h) - f(x_0)}{h}$$\n",
    "\n",
    "Производная  равна углу наклона касательной в точке $x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def deriv_demo(h, x0):\n",
    "\n",
    "    x = np.linspace(0.01, 20, 100)\n",
    "    f = lambda x: np.sin(x)/x\n",
    "\n",
    "    deriv = (f(x0+h) - f(x0))/h\n",
    "\n",
    "    tang = lambda x:  f(x0) + deriv*(x - x0)\n",
    "    \n",
    "    plt.plot(x, f(x), label='$f(x)$')\n",
    "    ylim = plt.ylim()\n",
    "\n",
    "    plt.plot(x,tang(x), label='tangent line (deriv = %f)'%deriv)\n",
    "\n",
    "    plt.scatter(x0, f(x0), marker='s', s=100)\n",
    "    plt.scatter(x0+h, f(x0+h), marker='s', s=100)\n",
    "\n",
    "    plt.ylim(ylim)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(deriv_demo, h=FloatSlider(min=0.0001, max=2, step=0.005), x0=FloatSlider(min=1, max=15, step=.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "* Находим экстремум\n",
    "* Если точка $x_0$ - экстремум функции, и в ней существует производная то $f'(x_0) = 0$\n",
    "* Что если экстремумов много?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Нам повезло, ведь функция ошибки линейной регрессии - выпуклая функция.\n",
    "* Локальный экстремум выпуклой функции = глобальному экстремому"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "\n",
    "* В многомерном случае все обобщается и переходят к производным по направлению и градиенту:\n",
    "$$ f'_v(x_0) = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_0+hv) - f(x_0)}{h} = \\frac{d}{dh}f(x_{0,1} + hv_1, \\dots, x_{0,d} + hv_d) \\rvert_{h=0}, \\quad ||v|| = 1 \\quad \\text{Производная по направлению}$$\n",
    "\n",
    "\n",
    "$$ \\frac{ \\partial f(x_1,x_2,\\dots,x_d)}{\\partial x_i} = \\lim\\limits_{h \\rightarrow 0}\\frac{f(x_1, x_2, \\dots, x_i + h, \\dots, x_d) - f(x_1, x_2, \\dots, x_i, \\dots, x_d)}{h} \\quad \\text{Частная производная}$$\n",
    "\n",
    "$$ \\nabla f = \\left(\\frac{\\partial f}{\\partial x_i}\\right),\\quad i=1\\dots d  \\quad \\text{Градиент =  вектор частных производных}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tiny Refresher\n",
    "* Распишем производную по направлению как производную сложной функции\n",
    "\n",
    "$$ f'_v(x_0) = \\frac{d}{dh}f(x_{0,1} + hv_1, \\dots, x_{0,d} + hv_d) \\rvert_{h=0} = \\sum_{i=1}^d \\frac{\\partial f}{\\partial x_i} \\frac{d}{dh} (x_{0,i} + hv_i) = \\langle \\nabla f, v \\rangle$$\n",
    "\n",
    "$$ \\langle \\nabla f, v \\rangle = || \\nabla f || \\cdot ||v|| \\cdot \\cos{\\phi} = || \\nabla f || \\cdot \\cos{\\phi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   ## Tiny Refresher\n",
    "\n",
    "* Произвольная по направлению максимальна, если направление совпадает с градиентом.\n",
    "* Градиент — направление наискорейшего роста функции\n",
    "* Антиградиент — направление наискорейшего убывания функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/dir-der.gif'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def sq_loss_demo():\n",
    "\n",
    "    beta0 = np.linspace(-8000, 6500, 100)\n",
    "    beta1 = np.linspace(5000, 20000, 100)\n",
    "\n",
    "    x = df_auto.loc[:, ['mileage']]\n",
    "    x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "    X = np.c_[x, np.ones(df_auto.shape[0])]\n",
    "\n",
    "\n",
    "    B0, B1 = np.meshgrid(beta0, beta1)\n",
    "    L = ((X.dot(np.r_[B0.reshape(1,-1), B1.reshape(1,-1)]) - df_auto.loc[:, 'price'].values.reshape(-1,1))**2).sum(axis=0)/(2*df_auto.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.view_init(40, 25)\n",
    "    ax.plot_surface(B0, B1, L.reshape(B0.shape), alpha=0.3,)\n",
    "    # ax.plot_(X, Y, Z)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    contour = ax.contour(B0, B1, L.reshape(B0.shape),)\n",
    "    plt.clabel(contour, inline=1, fontsize=10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "$$ L(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{i=1}^n(\\beta_0 + \\beta_1x_1^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "* Предположим мы выбрали какое-то начальное приближение $(\\hat{\\beta_0}, \\hat{\\beta_1})$\n",
    "* Его можно постараться улучшить - надо двигаться в сторону наискорейшего убывания функции (Антиградиента!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sq_loss_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = \\frac{1}{n} X^\\top(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta - \\alpha\\frac{\\partial L}{\\partial \\beta}$$\n",
    "\n",
    "* $\\alpha$ -  скорость спуска\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Псевдокод алгоритма\n",
    "\n",
    "```{python}\n",
    "1.function gd(X, alpha, epsilon):\n",
    "\n",
    "2. \tinitialise beta \n",
    "\n",
    "3. \tdo: \n",
    "\n",
    "4.      Beta = new_beta\n",
    "\n",
    "5.      new_Beta = Beta - alpha*grad(X, beta)\n",
    "\n",
    "6.\tuntil dist(new_beta, beta) < epsilon\n",
    "\n",
    "7.\treturn beta\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def grad_demo(iters=1, alpha=0.001):\n",
    "    \n",
    "    beta0 = np.linspace(-5000, 2500, 100)\n",
    "    beta1 = np.linspace(5000, 20000, 100)\n",
    "\n",
    "    x = df_auto.loc[:, ['mileage']]\n",
    "    x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "    y = df_auto.loc[:, 'price'].values\n",
    "    X = np.c_[x, np.ones(df_auto.shape[0])]\n",
    "\n",
    "\n",
    "    B0, B1 = np.meshgrid(beta0, beta1)\n",
    "    L = ((X.dot(np.r_[B0.reshape(1,-1), B1.reshape(1,-1)]) - y.reshape(-1,1))**2).sum(axis=0)/(2*df_auto.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.scatter(X[:,0], y)\n",
    "    \n",
    "    Beta, costs, Betas = gradient_descent_upd(X, y, alpha, tol=10**-3, max_iter=iters)\n",
    "    Betas = np.c_[Betas]\n",
    "    \n",
    "    X_1 = np.sort(X, axis=0)\n",
    "    \n",
    "    y_hat = X_1.dot(Betas.T)\n",
    "    \n",
    "    plt.plot(X_1[:,0], y_hat)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    contour = ax.contour(B0, B1, L.reshape(B0.shape),)\n",
    "    plt.clabel(contour, inline=1, fontsize=10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    \n",
    "    ax.plot(Betas[:,0], Betas[:, 1], '*-')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def stoch_grad_demo(iters=1, alpha=0.001):\n",
    "    \n",
    "    beta0 = np.linspace(-8000, 5500, 100)\n",
    "    beta1 = np.linspace(5000, 20000, 100)\n",
    "\n",
    "    x = df_auto.loc[:, ['mileage']]\n",
    "    x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "    y = df_auto.loc[:, 'price'].values\n",
    "    X = np.c_[x, np.ones(df_auto.shape[0])]\n",
    "\n",
    "\n",
    "    B0, B1 = np.meshgrid(beta0, beta1)\n",
    "    L = ((X.dot(np.r_[B0.reshape(1,-1), B1.reshape(1,-1)]) - y.reshape(-1,1))**2).sum(axis=0)/(2*df_auto.shape[0])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 7))    \n",
    "    Beta, costs, Betas = gradient_descent_upd(X, y, alpha, tol=10**-3, max_iter=iters)\n",
    "    Betas = np.c_[Betas]\n",
    "    \n",
    "    _, _, Betas_stoch = stoch_gradient_descent(X, y, alpha, max_iter=iters)\n",
    "    Betas_stoch = np.c_[Betas_stoch]\n",
    "    \n",
    "    X_1 = np.sort(X, axis=0)\n",
    "    \n",
    "    y_hat = X_1.dot(Betas.T)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    contour = ax.contour(B0, B1, L.reshape(B0.shape),)\n",
    "    plt.clabel(contour, inline=1, fontsize=10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    \n",
    "    ax.plot(Betas[:,0], Betas[:, 1], '*-')\n",
    "    ax.plot(Betas_stoch[:,0], Betas_stoch[:, 1], 'o-', c='b')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def gradient_descent_upd(X, y, alpha, tol=10**-3, max_iter=10):\n",
    "    n = y.shape[0]\n",
    "    Beta = np.array([-4000, 6000])\n",
    "    delta = 10\n",
    "    cost_prev = 0\n",
    "    i = 0\n",
    "    \n",
    "    # for logging\n",
    "    Betas = [Beta]\n",
    "    costs = []\n",
    "    \n",
    "    while (delta > tol) and (i <= max_iter):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # считаем ошибку и значение функции потерь\n",
    "        error = (y_hat - y)\n",
    "        cost = np.sum(error ** 2)/float(2 * n)\n",
    "        delta = abs(cost - cost_prev)\n",
    "        cost_prev = cost\n",
    "        \n",
    "        # считаем градиент\n",
    "        grad = X.T.dot(error) / n\n",
    "\n",
    "        # обновляем коэффициенты\n",
    "        Beta = Beta - alpha * grad\n",
    "        \n",
    "        # logging\n",
    "        if i % 5 == 0:\n",
    "            costs.append(cost)\n",
    "            Betas.append(Beta)\n",
    "        i += 1\n",
    "        \n",
    "    return Beta, costs, Betas\n",
    "\n",
    "\n",
    "def stoch_gradient_descent(X, y, alpha, max_iter=10):\n",
    "    n = y.shape[0] \n",
    "    Beta = np.array([-4000, 6000])\n",
    "    \n",
    "    costs = []\n",
    "    Betas = [Beta]\n",
    "    \n",
    "    for i in xrange(max_iter):\n",
    "        \n",
    "        X, y = shuffle(X, y, random_state=10)\n",
    "        \n",
    "        for j in range(n):\n",
    "            \n",
    "            y_hat = X[j].dot(Beta)\n",
    "\n",
    "            # считаем ошибку и значение функции потерь\n",
    "            error = y_hat - y[j]\n",
    "\n",
    "            # считаем градиент\n",
    "            gradient = X[j].T.dot(error)\n",
    "\n",
    "            # обновляем коэффициенты\n",
    "            Beta = Beta - alpha * gradient  # update\n",
    "            alpha *= 0.99\n",
    "                # logging\n",
    "            if j % 5 == 0 and i % 5 == 0:\n",
    "                Betas.append(Beta)\n",
    "        \n",
    "        cost_epoch = np.sum((X.dot(Beta) - y)**2 / (2*n))\n",
    "        costs.append(cost_epoch)\n",
    "        \n",
    "        \n",
    "    return Beta, costs, Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grad_demo(iters=105, alpha=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Нюансы\n",
    "\n",
    "* Надо подбирать $\\alpha$\n",
    "* Влияние шкал признаков\n",
    "* Попадание в локальный минимум*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вариации градиентного спуска\n",
    "* Стохастический градиентрый спуск (!)\n",
    "* Градиентный спуск с адаптивной скоростью $\\alpha$\n",
    "* Корректировка на импульс\n",
    "* ...\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/contours_evaluation_optimizers.gif'></th>\n",
    "    <th class=\"tg-031e\"><img src='http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif'></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "[Обзор методов 1](http://sebastianruder.com/optimizing-gradient-descent/), [обзор 2](https://medium.com/towards-data-science/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Стохастический градиентный спуск\n",
    "\n",
    "```{python}\n",
    "1.function sgd(X, alpha, epsilon):\n",
    "\n",
    "2. \tinitialise beta \n",
    "\n",
    "3. \tdo: \n",
    "\n",
    "4.        X = shuffle(X)\n",
    "\n",
    "5.        for x in X:\n",
    "\n",
    "6.            Beta = new_beta\n",
    "\n",
    "7.            new_Beta = Beta - alpha*grad(x, beta)\n",
    "\n",
    "8.\tuntil dist(new_beta, beta) < epsilon\n",
    "\n",
    "9.\treturn beta\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stoch_grad_demo(iters=105, alpha=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Так же есть\n",
    "* Методы 0-го порядка\n",
    "    * Покоординатный спуск\n",
    "* Методы 2-го порядка\n",
    "    * Ньютоновские методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Основные факторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Переобучение\\недообучение\n",
    "\n",
    "<center><img src=http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20[8].png></center>\n",
    "[Andrew's Ng Machine Learning Class - Stanford]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Мультиколлинеарность\n",
    "\n",
    "**Мультиколлинеарность** - эффект при котором, подмножество предикторов являются (почти) линейно зависимыми (коэффициент корреляции по модулю близок к 1). Из-за этого:\n",
    "\n",
    "* Матрица $X^{\\top} X$ становится плохо обусловленной или необратимой\n",
    "* Зависимость $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$ перестаёт быть одназначной\n",
    "\n",
    "С этим эффектом можно бороться несколькими способами\n",
    "\n",
    "* Последовательно добавлять переменные в модель\n",
    "* Исключать коррелируемые предикторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Регуляризация\n",
    "\n",
    "В обоих случаях может помочь **регуляризация** - добавление штрафного слагаемого за сложность модели в функцию потерь. В случае линейной регрессии было:\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(a(x^{(i)}) - y^{(i)})^2 $$\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)}) - y^{(i)})^2\\right] + \\frac{1}{C}\\sum_{j=1}^{m}\\beta_j^2$$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)})  - y^{(i)})^2\\right] + \\frac{1}{C}\\sum_{j=1}^{m}|\\beta_j| $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=images/regul.jpg></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Природа зависимости\n",
    "\n",
    "Далеко не всегда переменные зависят друг от друга именно в том виде, в котором они даны. Никто не запрещает зависимость вида\n",
    "$$\\log(y) = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\frac{1}{x_1}$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1\\log(x_1)$$\n",
    "или\n",
    "$$y = \\beta_0 + \\beta_1 x_1^2 + \\beta_2 x_2^2 + \\beta_3 x_1x_2 $$\n",
    "и т.д.\n",
    "\n",
    "Не смотря на то, что могут возникать какие-то нелинейные функции - всё это сводится к **линейной** регрессии (например, о втором пункте, произведите замену $z_1 = \\frac{1}{x_1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def demo_weights():\n",
    "    df = pd.read_csv('weights.csv', sep=';', index_col=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    df.plot(x = 'body_w', y='brain_w', kind='scatter', ax=ax[0])\n",
    "    for k, v in df.iterrows():\n",
    "        ax[0].annotate(k, v[:2])\n",
    "    # Должно получится что-то несуразное..\n",
    "    \n",
    "    df['log_body_w'] = np.log(df.body_w)\n",
    "    df['log_brain_w'] = np.log(df.brain_w)\n",
    "    df.plot(x = 'log_body_w', y='log_brain_w', kind='scatter', ax=ax[1])\n",
    "    for k, v in df.iterrows():\n",
    "        ax[1].annotate(k, v[2:])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demo_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Логистическая  регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/512px-Svm_separating_hyperplanes_%28SVG%29.svg.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Нам надо найти уравнение прямой (гиперплоскости), которая бы могла разделить два класса ($H_2$ и $H_3$ подходят). В данном случае, уравнение прямой задаётся как: $$g(x) = w_0 + w_1x_1 + w_2x_2 = \\langle w, x \\rangle + w_0 =  w^\\top x + w_0$$\n",
    "\n",
    "* Если $g(x^*) > 0$, то $y^* = \\text{'черный'} = +1$\n",
    "* Если $g(x^*) < 0$, то $y^* = \\text{'белый'} = -1$\n",
    "* Если $g(x^*) = 0$, то мы находимся на линии\n",
    "* т.е. решающее правило: $y^* = sign(g(x^*))$\n",
    "\n",
    "Некоторые геометрические особенности\n",
    "* $\\frac{w_0}{||w||}$ - расстояние от начала координат то прямой\n",
    "* $\\frac{|g(x)|}{||w||}$ - степень \"уверенности\" в классификациий\n",
    "* Величину $M = y \\cdot g(x)$ называют **отступом**(margin)\n",
    "\n",
    "Если для какого-то объекта $M \\geq 0$, то его классификация выполнена успешно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Отлично! Значит нам надо просто минимизировать ошибки классификации для всех объектов:\n",
    "\n",
    "$$L(w) = \\sum_i [y^{(i)} g(x^{(i)}) < 0] \\rightarrow \\min_w$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Проблема в том, что это будет комбинаторная оптимизация. Существуют различные аппроксимации этой функции ошибок:\n",
    "<center><img src='http://jaquesgrobler.github.io/Online-Scikit-Learn-stat-tut/_images/plot_sgd_loss_functions_11.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def demo_sigmoid():\n",
    "    z = np.linspace(-10, 10, 100)\n",
    "\n",
    "    y = sigmoid(z)\n",
    "    plt.plot(z, y)\n",
    "    plt.xlabel('$z$')\n",
    "    plt.ylabel('$\\sigma(z)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Перед тем как мы начнем, рассмотрим функцию $$\\sigma(z) = \\frac{1}{1 + exp{(-z)}},$$она называется **сигмойда**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1./(1+np.exp(-z))\n",
    "demo_sigmoid() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Постановка задачи\n",
    "Будем требовать, чтобы алгоритм возвращал вероятность класса $y=+1$:\n",
    "$$h(x,w) = p(y=+1|x,w) = \\sigma(g(x))$$\n",
    "\n",
    "Выпишем функцию правдоподобия\n",
    "$$ \\mathcal{L}(w) = \\prod_i^n h(x^{(i)},w)^{[y^{(i)} = +1]} (1 - h(x^{(i)},w))^{[y^{(i)} = -1]} \\rightarrow \\max_w$$\n",
    "$$ -\\log{\\mathcal{L}(w)} = - \\sum_i^n [y^{(i)} = +1]\\cdot\\log{(h(x^{(i)},w))} + {[y^{(i)} = -1]}\\cdot\\log{(1-h(x^{(i)},w))} \\rightarrow \\min_w$$\n",
    "$$L(w) = \\log{\\mathcal{L}(w)} \\rightarrow \\min_w $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рассмотрим объекты со следующими признаками:\n",
    "\n",
    "| x1 | x2 |\n",
    "|---|---|\n",
    "| 0 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 2 |\n",
    "\n",
    "Определите к какому классу относятся наблюдения, если вектор параметров имеет вид $(w_0 = -0.3 , w_1 = 0.1, w_2 = 0.1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,1],[1,0],[1,1],[2,2],[2,3],[3,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Другая постановка\n",
    "Можно несколькими способами представить логистическую регрессию:\n",
    "\n",
    "Рассмотрим принадлежность к классу $y=\\pm1$ некого объекта $x$: \n",
    "$$p(y=\\pm1 | x,w)$$ и выразим её через **сигмойду** от **отступа**:\n",
    "\n",
    "$$p(y=\\pm1|x,w) = \\sigma(y (\\langle w, x \\rangle + w_0)) = \\sigma(y^{(i)} g(x^{(i)}))$$\n",
    "\n",
    "А ошибка, которую мы будем минимизировать - логарифмическая:\n",
    "\n",
    "$$L(w) = - \\frac{1}{N}\\sum_i \\log(\\sigma(y^{(i)} g(x^{(i)})) = - \\frac{1}{N}\\sum_i \\log(1 - e^{-y^{(i)} g(x^{(i)})}) \\rightarrow \\min_w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/overfitting.jpg'></center>\n",
    "* Как искать $w$? - c помощью градиентного спуска!\n",
    "* А что с регуляризацией? - та же история \n",
    "    * Lasso: $ \\frac{1}{C} \\sum\\limits_{j=1}^d |w_j|$\n",
    "    * Rigde: $ \\frac{1}{C} \\sum\\limits_{j=1}^d w_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* А если классов несколько?\n",
    "    * 1-vs-1\n",
    "    * 1-vs-rest\n",
    "    * Softmax :\n",
    "$$ p(y=j|W, x )={\\frac {e^{ x ^{\\mathsf {T}} w _{j}}}{\\sum _{k=1}^{K}e^{x ^{\\mathsf {T}} w_{k}}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вопросы?\n",
    "\n",
    "## Оставьте, пожалуйста, свой отзыв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1024px"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "none",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
