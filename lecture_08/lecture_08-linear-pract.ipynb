{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/header.png\">\n",
    "\n",
    "<h1><center>Алгоритмы интеллектуальной обработки больших объемов данных</center></h1>\n",
    "<hr>\n",
    "<h2><center>Линейные модели (практика)</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed\n",
    "except ImportError:\n",
    "    print u'Так надо'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример: Стоимость автомобиля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите [тренировочные данные](http://bit.ly/1gIQs6C) и [тестовые данные](http://bit.ly/IYPHrK) - уже знакомые нам данные по автомобилям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://bit.ly/1gIQs6C')\n",
    "df_test = pd.read_csv('http://bit.ly/IYPHrK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='mileage', y='price', kind='scatter', s=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что между стоимостью и пробегом зависимость линейная - давайте ее найдем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Модель:\\nprice = %.2f + (%.2f)*mileage' % (model.intercept_, model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуйте предсказание модели (прямую) вместе с данными на плоскости. Здесь можно либо явно взять уравнение прямой и посчитать значения в каждой точке, либо через predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.plot(x='mileage', y='price', kind='scatter', s=120)\n",
    "\n",
    "## Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Меры качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посчитать простые варианты агрегирования остатков, например:\n",
    "\n",
    "* $\\frac{1}{n} \\sum_i |\\hat{y}^{(i)}-y^{(i)}|$ - средняя абсолютная ошибка\n",
    "* $\\frac{1}{n} \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$ - средняя квадратичная ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Средняя абсолютная ошибка %.2f' % mean_absolute_error(y_train, y_hat))\n",
    "print('Средняя квадратичная ошибка %.2f' % mean_squared_error(y_train, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно рассмотреть более сложную меру: коэффициент детерминации $R^2$:\n",
    "\n",
    "* $TSS = \\sum_i (y^{(i)}-\\bar{y})^2$ - общая сумма квадратов (total sum of squares)\n",
    "* $RSS = \\sum_i (\\hat{y}^{(i)}-y^{(i)})^2$ - сумма квадратов остатков (residual sum of squares)\n",
    "* $ESS = \\sum_i (\\hat{y}^{(i)}-\\bar{y})^2$ - объясненная сумма квадратов (explained sum of squares)\n",
    "\n",
    "Для простоты будем считать, что\n",
    "$$TSS = ESS + RSS$$\n",
    "\n",
    "Тогда Коэффициент детерминации $R^2=1-\\frac{RSS}{TSS}$\n",
    "\n",
    "Рассчитайте его для нашей модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование переменных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переход к близким или единым шкалам улучшает сходимость градиентного спуска, уменьшает риск переполнения разрядности чисел, однако приходится жертвовать прямой интерпретируемостью..\n",
    "\n",
    "Нормализацию обычно проделывают для вещественных признаков.\n",
    "\n",
    "Нормализация z-score:\n",
    "1. Вычитаем среднее: $x - \\bar{x}$\n",
    "2. Делим на стандартное отклонение: $\\frac{x - \\bar{x}}{std(x)}$\n",
    "\n",
    "Можно проделать вручную, можно с помошью метода ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Модель:\\nprice = %.2f + (%.2f)*mileage`' % (model.intercept_, model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как обучать линейную регрессию?\n",
    "Попробуем разобраться без всяких `.predict()` и `.fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим случай с одним признаком и свободным членом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ - признаковое описание наблюдений,<br\\> $y$ - прогнозируемая величина\n",
    "\n",
    "Пусть задана функция ошибки (функция потерь) $L(\\cdot)$. <br\\>\n",
    "Нам надо построить такой функционал $f(X)$, который будет выдавать значение наиболее близкие к $y$, иначе говоря: $$L\\left(f(X) - y\\right) \\rightarrow\\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию потерь, как сумму квадратов разности выдаваемого ответа функционала и реального значения: \n",
    "$$ L(\\cdot) = \\frac{1}{2n}\\sum_{i=1}^n(f(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Так как среди всего множества моделей мы выбрали линейную регрессию, то $$f(X) = \\beta_0 + \\beta_1x_1$$\n",
    "Подставляем это выражение в $L(\\cdot)$ и находим $\\beta_0$,\n",
    "$\\beta_1$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L(\\beta_0,\\beta_1) = \\frac{1}{2n}\\sum^{n}_{i=1}(f(x^{(i)})  - y^{(i)})^2 = \\frac{1}{2n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})^2  \\rightarrow \\min\\limits_{\\beta_0, \\beta_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим функцию потерь на трехмерном графике в зависимости от $\\beta_0$ и $\\beta_1$ для задачи с автомобилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# для удобства добавим столбец из \"1\" в матрицу с признаком \"пробег\"\n",
    "X_model = np.c_[np.ones(X_train.shape), X_train]\n",
    "X_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.linspace(11000 , 13000, 100)\n",
    "beta1 = np.linspace(-2450, -250, 100)\n",
    "\n",
    "B0, B1= np.meshgrid(beta0, beta1)\n",
    "\n",
    "B_all = np.c_[B0.reshape(-1,1), B1.reshape(-1,1)].T\n",
    "\n",
    "L = X_model.dot(B_all) - y_train.reshape(-1,1)\n",
    "L = L ** 2\n",
    "L = L.mean(axis=0)/2\n",
    "L = L.reshape(B0.shape)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.view_init(40, 25)\n",
    "ax.plot_surface(B0, B1, L, alpha=0.3,)\n",
    "ax.set_xlabel('beta_0')\n",
    "ax.set_ylabel('beta_1')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "contour = ax.contour(B0, B1, L)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "ax.set_xlabel('beta_0')\n",
    "ax.set_ylabel('beta_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентый спуск - это итеративный метод оптимизации функции. Он заключается в постепенном перемещении к точке экспетмума в направлении антиградиента этой функции в точке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем, чему равен градиент функции потерь $L(\\beta_0, \\beta_1):$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})$$\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_1} = \\frac{1}{n}\\sum^{n}_{i=1}(\\beta_0 + \\beta_1x^{(i)}_1 - y^{(i)})x_1^{(i)}$$\n",
    "\n",
    "Иногда проще это записать в виде матриц:\n",
    "$$ \\frac{\\partial L}{\\partial \\beta} = X^\\top(X\\beta - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска заключается в итеративном и **одновременном(!!!)** обновлении значений $\\beta$ в направлении, противоположному градиенту:\n",
    "$$ \\beta := \\beta -  \\frac{\\alpha}{n} \\frac{\\partial L}{\\partial \\beta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь к шагам алгоритма:\n",
    "\n",
    "* Задаем случайное начальное значение для $\\beta$\n",
    "* Пока не будет достигнуто правило останова:\n",
    "    * Считаем ошибку и значение функции потерь\n",
    "    * Считаем градиент\n",
    "    * Обновляем коэффициенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, iters, alpha):\n",
    "    \n",
    "    costs = []\n",
    "    betas = []\n",
    "    \n",
    "    n = y.shape[0] \n",
    "    Beta = np.random.rand(X.shape[1])\n",
    "    for i in xrange(iters):\n",
    "        y_hat = X.dot(Beta)\n",
    "        \n",
    "        # считаем ошибку и значение функции потерь\n",
    "        \n",
    "        # считаем градиент\n",
    "\n",
    "        # обновляем коэффициенты\n",
    "        # Beta = Beta - alpha/n * grad\n",
    "                    \n",
    "    return Beta, costs, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Beta, costs, betas = gradient_descent(X_model, y_train, 100, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta0 = np.linspace(11000 , 13000, 100)\n",
    "beta1 = np.linspace(-2450, -250, 100)\n",
    "\n",
    "B0, B1= np.meshgrid(beta0, beta1)\n",
    "\n",
    "B_all = np.c_[B0.reshape(-1,1), B1.reshape(-1,1)].T\n",
    "\n",
    "L = X_model.dot(B_all) - y_train.reshape(-1,1)\n",
    "L = L ** 2\n",
    "L = L.mean(axis=0)/2\n",
    "L = L.reshape(B0.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "contour = ax.contour(B0, B1, L)\n",
    "plt.clabel(contour, inline=1, fontsize=10)\n",
    "ax.set_xlabel('beta_0')\n",
    "ax.set_ylabel('beta_1')\n",
    "\n",
    "betas = np.array(betas)\n",
    "ax.plot(betas[:,0], betas[:,1], marker='*')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Квадратичная ошибка достаточно чувствительна к выбросам. Давайте вернемся к нашим данным про автомобили и добавим туда выбросы.\n",
    "\n",
    "Посмотрим, как поведет себя простая линейная регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('http://bit.ly/1gIQs6C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.mileage.values.reshape(-1, 1)\n",
    "y_train = df_train.price.values\n",
    "\n",
    "n = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Добавляем выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    X_train = np.r_[X_train, [[250000+np.random.rand()*10000]]]\n",
    "    y_train = np.r_[y_train, 16000+np.random.randn()*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train[:n], y_train[:n])\n",
    "\n",
    "model_ouliers = LinearRegression(fit_intercept=True)\n",
    "model_ouliers.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model.predict(x)\n",
    "y_hat_outliers = model_ouliers.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red', label='good model')\n",
    "ax.plot(x, y_hat_outliers, c='green', label='biased model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея метода RANdom SAmple Consensus (RANSAC) заключается в многократном обучении модели на случайном наборе точек из исходных данных с последующим выбором лучшей модели.\n",
    "\n",
    "То есть:\n",
    "* Задаем функцию потерь\n",
    "* Задаем порог $\\theta$ для остатков при котором наблюдения начинают относится к выбросам\n",
    "* Задаем правило останова\n",
    "\n",
    "Шаги алгоритма следующие\n",
    "1. Взять случайные K точек и обучить на них модель M\n",
    "2. Сравнить ошибки на остальных точких с порогом $\\theta$ и отнести к выбросам или внутренним точкам\n",
    "3. Обучить модель на всех внутренних точках, оценить качество на внутренних точках\n",
    "4. Повторить 1-3 пока не наступит правило останова. \n",
    "5. Вывод: модель с лучшим качеством"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ransac = RANSACRegressor(LinearRegression())\n",
    "model_ransac.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model_ransac.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red', label='RANSAC model')\n",
    "ax.plot(x, y_hat_outliers, c='green', label='biased model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея робастных методов заключается во взвешивании остатков модели таким образом, чтобы большие значения вносили меньший вклад в оценку параметров.\n",
    "\n",
    "Таким образом, вместо минимизации квадрата остатков $$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "Будут минимизироваться взвешенные остатки $$ L_w(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\sum^{n}_{i=1}\\rho(\\hat{y}^{(i)} - y^{(i)}),$$\n",
    "где $\\rho(\\cdot)$ - некоторая взвешивающая функция.\n",
    "\n",
    "Для того, чтобы попробовать эти методы нужно будет устновить пакет `statsmodels` через `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 4.685\n",
    "support = np.linspace(-3*c, 3*c, 1000)\n",
    "tukey = sm.robust.norms.TukeyBiweight(c=c)\n",
    "plt.plot(support, tukey(support))\n",
    "plt.ylim(.1, -4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_robust = sm.RLM(y_train, sm.add_constant(X_train), M=sm.robust.norms.TukeyBiweight())\n",
    "model_robust = model_robust.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_robust.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, max(X_train), 100).reshape(-1, 1)\n",
    "y_hat = model_robust.predict(sm.add_constant(x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.scatter(X_train, y_train)\n",
    "\n",
    "ax.plot(x, y_hat, c='red', label='Robust model')\n",
    "ax.plot(x, y_hat_outliers, c='green', label='biased model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Игрушечный пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем выборку и опробуем логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) + [2, 2],\n",
    "          np.random.randn(20, 2) + [-2, -2]]\n",
    "y = [-1] * 20 + [1] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X[:, 0],\n",
    "           X[:, 1],\n",
    "           c=y,\n",
    "           cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию на этих данных и нарисуем разделяющую гиперплоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=1.0, \n",
    "                           fit_intercept=True, \n",
    "                           penalty='l2')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'w_0 = %f' % model.intercept_\n",
    "print 'w_1, w_2 = ', model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нарисуем эту гиперплоскость\n",
    "w_0 = model.intercept_[0]\n",
    "w_1 = model.coef_[0][0]\n",
    "w_2 = model.coef_[0][1]\n",
    "\n",
    "x_1 = np.linspace(-4, 4, 10)\n",
    "x_2 = - (w_0 + w_1*x_1)/w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X[:, 0],\n",
    "           X[:, 1],\n",
    "           c=y,\n",
    "           cmap=plt.cm.Paired)\n",
    "plt.plot(x_1, x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = model.predict(X)\n",
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat_proba = model.predict_proba(X)\n",
    "y_hat_proba[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_func = model.decision_function(X)\n",
    "dec_func[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как сделать нелинейную границу?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим набор данных, который в простонародье называют \"Бублик\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=100, shuffle=True,\n",
    "                    noise = 0.1,\n",
    "                    factor=0.1)\n",
    "\n",
    "plt.scatter(X[:, 0],\n",
    "            X[:, 1],\n",
    "            c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что классы нельзя разделить линией. Но можно сделать это окружностью! </br>\n",
    "Т.е. разделяющся линия теперь будет задаваться не уравнением прямой $g(x) = w_0 + w_1x_1 + w_2x_2$, а уравнением окружности. \n",
    "\n",
    "Выполните преобразование матрицы X, чтобы в ней были столбцы для $x_1$, $x_2$, $x^2_1$ + $x^2_2$ и обучите логистическую регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code Here\n",
    "X_new = ...\n",
    "model = LogisticRegression(C=100000, \n",
    "                           fit_intercept=True)\n",
    "model.fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Посчитаем количество ошибок\n",
    "y_hat = model.predict(X_new)\n",
    "(y != y_hat).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Нарисуем полученную окружность\n",
    "\n",
    "x0, x1 = np.meshgrid(np.arange(-1.5, 1.5, 0.1),\n",
    "                       np.arange(-1.5, 1.5, 0.1))\n",
    "xx0, xx1 = x0.ravel(), x1.ravel()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ тональности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите текстовые данные [отсюда](https://archive.ics.uci.edu/ml/machine-learning-databases/00331/). Архив должен содержать 3 файла с положительными и отрицательными отзывами с ресурсов\n",
    "* imdb.com\n",
    "* amazon.com\n",
    "* yelp.com\n",
    "\n",
    "Формат файла следующий:\n",
    "<отзыв>\\t<метка>\\n\n",
    "\n",
    "\n",
    "### Задача\n",
    "1. Загрузите тексты и метки классов в разные переменные\n",
    "2. Выберите меру качества классификации\n",
    "3. Обучите логистическую (без подбора гиперпараметров). Тексты представляются в виде мешка слов\n",
    "4. Выведите наиболее значимые слова из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_data = pd.read_csv('sentiment/yelp_labelled.txt', sep='\\t', names=['sentence', 'label'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sent_data.sentence.values\n",
    "y = sent_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кредитный скорринг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим данные по кредитованию.\n",
    "\n",
    "Столбец с классом называется `y`.<br/> Значение $1$ соответствует классу клиентов банка, которым выдали кредит и они его успешно вернули.<br/> Значение $-1$ соответствует клиентам, невыполнившим свои кредитные обязанности. \n",
    "\n",
    "В банке хотят уметь определять по признакам `a1-a15`, сможет ли новый клиент вернуть кредит или нет? То есть нам надо обучить классификатор! *Обычно, в банках используют скор-карты, но процесс их построения тесно связан с логистической регрессией*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные и преобразуйте признаки `a1`, `a9`, `a10` и `a12` из строковых в числовые. В них только 2 возможных значения. Для этого можно использовать функцию DataFrame.replace() в `pandas` или самое обычное присваивание на соответствующих строках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('crx.data')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заполнение пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполните пропуски в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.a1 = df.a1.fillna(value=df.a1.mode().values[0])\n",
    "df.a2 = df.a2.fillna(value=df.a2.median())\n",
    "\n",
    "df.a4 = df.a4.fillna(value=df.a4.mode().values[0])\n",
    "df.a5 = df.a5.fillna(value=df.a5.mode().values[0])\n",
    "df.a6 = df.a6.fillna(value=df.a6.mode().values[0])\n",
    "df.a7 = df.a7.fillna(value=df.a7.mode().values[0])\n",
    "\n",
    "df.a14 = df.a14.fillna(value=df.a14.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаках `a6`, `a7` присутствуют \"редкие\" значение. Найдите их с помощью фунцкии `.value_counts()`  и объедините, присвоив им одно и то же значение, например `'Other'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.a7.replace({'z': 'Other', 'j': 'Other', 'dd': 'Other',\n",
    "               'n': 'Other', 'o': 'Other'}, inplace=True)\n",
    "\n",
    "df.a6.replace({'j': 'Other', 'r': 'Other'}, inplace=True)\n",
    "\n",
    "# df.a6 = af.a6.replace({'j': 'Other', 'r': 'Other'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Влияние категориальной (да и в общем-то любой) переменной на целевую переменную можно с помощью разных метрик.\n",
    "\n",
    "Для категорий есть мера [Normalized Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделите бинарные признаки `a1`, `a9`, `a10` и `a12` в матрицу `X_binary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.a1.replace({'a': 1, 'b': 0}, inplace=True)\n",
    "df.a9.replace({'t': 1, 'f': 0}, inplace=True)\n",
    "df.a10.replace({'t': 1, 'f': 0}, inplace=True)\n",
    "df.a12.replace({'t': 1, 'f': 0}, inplace=True)\n",
    "\n",
    "X_binary= df.loc[:, ['a1', 'a9', 'a10','a12']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На занятиях мы изучатли преобразование категориальных признаков с помощью комбинации `LabelEncoder` и `OneHotEncoder` или метода `pd.get_dummies`. <br\\>\n",
    "\n",
    "Есть еще один способ из библиотеки sklearn - `DictVectorizer`. Если хотите - можете разобраться, что делает код ниже, изучая промежуточные результаты каждой из команд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "dv.fit(df[['a5', 'a6', 'a7', 'a13']].T.to_dict().values())\n",
    "\n",
    "X_cat = dv.transform(df[['a5', 'a6', 'a7', 'a13']].T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_cat` - эквивалентна последовательному применению `LabelEncoder` к каждому признаку `a5`, `a6`, `a7`, `a13` и использованию `OneHotEncoder`. Чтобы узнать, чему соответствует каждый столбец матрицы `X_cat` используйте команду `dv.feature_names_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы долнжы увидеть список с элементами вида <Признак>=<Значение>. Если в каком-то из случаев это не так, то\n",
    "* Либо `DictVectorizer` посчитал, что признак - некатегориальный\n",
    "* Либо в признаке есть пропуски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода  df[..].corr() убедитесь, что признаки `a2`, `a3`, `a8`, `a11`, `a14` и `a15` довольно слабо коррелируют друг с другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['a2', 'a3', 'a8', 'a11', 'a14', 'a15']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуйте количественные признаки `a2`, `a3`, `a8`, `a11`, `a14` и `a15` с помощью `StandartScaler` или вручную. Вы должны получить матрицу `X_real`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**МЕТОДОЛОГИЧЕСКИЙ КОСЯК:** нужно сначала разбить на трейн и тест, а потом использовать всякое шкалирование и кодирование. Но для целей этого задания - это не так вредно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_real = scaler.fit_transform(df[['a2', 'a3', 'a8', 'a11', 'a14', 'a15']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте функцию `np.concatinate(..)` или `np.c[..]` чтобы сцепить матрицы `X_binary`, `X_cat` и `X_real`\n",
    "\n",
    "В результате вы должны получить матрицу с преобразованными призанками `X` и вектор ответов `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[X_binary, X_cat, X_real]\n",
    "y = df.loc[:, 'y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо мультиколлинеарности логистическая регрессия так же может быть подвержена переобучению. Как правило, на деле это выражается в \n",
    "* Очень больших весах $|w_i|$ при признаках\n",
    "* Неустойчивости решения (особенно в случае мультиколлинеарности, когда возникает линейная зависимость признаков и получается целое семейство равнозначных моделей)\n",
    "* Большая разница в качестве на обучении и валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация - это техника понижения \"сложности\" модели с минимальными последствиями для её качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с логистичесткой регресии, сложность модели выражается в значениях весов $w_j$ при признаках. Больший вес означает большее влияние признака на результат. В таком случае, давайте добавил штрафное слагаемое в функцию оптимизации для логистической регресии. Самый распространенные из них это:\n",
    "\n",
    "Стало (Ridge Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)}) - y^{(i)})^2\\right] + \\frac{1}{C}\\sum_{j=1}^{m}\\beta_j^2 $$\n",
    "или (Lasso Regularization)\n",
    "$$ L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2n}\\left[ \\sum^{n}_{i=1}(a(x^{(i)})  - y^{(i)})^2\\right] + \\frac{1}{C}\\sum_{j=1}^{m}|\\beta_j| $$\n",
    "\n",
    "$C$ - называется гиперпараметром регуляризации и он задается вручную. Выбирается он с помощью кросс-валидации. Чем больше $С$ - тем меньше влияние регуляризации.\n",
    "\n",
    "Lasso regression называется так, потому что она осуществляет \"отлов\" признаков - т.е. незначимые признаки будут иметь нулевые веса в модели, в то время как в Ridge regression - веса будут постепенно падать у всех признаков.\n",
    "\n",
    "<img src='http://webdancer.is-programmer.com/user_files/webdancer/Image/lasso.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако использование регуляризации превращает логистическую регрессию в черный ящик - мы предоставляем оптимизационному методу решать за нас, какие признаки важны для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сравним работу регуляризаторов. \n",
    "\n",
    "1. Разбейте данные на обучающую и контрольную выборки.\n",
    "1. Для $C$ из набора np.logspace(-3, 3, 10) обучите LogisctigRegression c Lasso регуляризацией (`penalty='l1'`). На каждой итерации оцените качество (ROC-AUC) на контрольной выборке и запомните полученные коэффициенты модели\n",
    "1. На одном графике выведите значение качества в зависимости от параметра `C` \n",
    "1. На другом графике для каждого признака выведите изменение коэффициента в модели в зависимости от параметра `C`\n",
    "1. Для оптимальной на ваш взгляд настройки модели выведите 5 наиболие \"важных\" признаков и их коэффициенты\n",
    "1. Проделайте тоже самое для Ridge регуляризации (`penalty='l2'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l1', C=1, fit_intercept=True)\n",
    "#... После fit()\n",
    "\n",
    "# # Коэффициенты w_1...w_d\n",
    "# model.coef_\n",
    "\n",
    "# # Коэффициент при свободном члене w_0\n",
    "# model.intercept_\n",
    "\n",
    "# # Предсказание\n",
    "# model.predict(X_test)\n",
    "# model.predict_proba(X_test)\n",
    "# model.decision_function(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = np.empty((X.shape[1],))\n",
    "scores = []\n",
    "\n",
    "c_range = np.logspace(-3, 3, 10)\n",
    "\n",
    "for C in c_range:\n",
    "# Your Code Here    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так (примерно) выглядит скор-карта на выдачу кредита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.mathworks.com/help/finance/credit_score_card_overview.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый \"признак\" разбит на бины (bins) - значимые интервалы. Если значение признака попадает в интервал, то к скору заемщика прибавляется сооветвтвующая величина.\n",
    "\n",
    "Таким образом формируется итоговый скор и далее принимается решение о выдаче кредита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка порого бинов для признаков - это отдельная наука. Скоррингисты используют так называемое IV (information value) для оценки качества разбиения признака на бины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы изучили с вами другой способ выявления порогов на признаках - деревья решений (случайный лес). Почему бы нам это не использовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "* Разбейте выборку на 2 части (обучение и контроль)\n",
    "* На половине обучающей выборке обучите случайный лес\n",
    "* С помощью метода `rand_forest.apply(X)` можно получить матрицу размера $n \\times t$, где $t$ - количество деревьев в лесу. В эту матрицу будет числом записан номер листа, в который попал объект\n",
    "* One-Hot Encoding!\n",
    "* Обучите логистическую регрессию на второй половине обучающей выборки\n",
    "* Сравние roc-auc на обычной лог-регрессии, обычного случайного леса и их композиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_emb, y_train2, y_emb = train_test_split(X_train, y_train, \n",
    "                                                    test_size = 0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "rand_forest.fit(X_emb, y_emb)\n",
    "\n",
    "X_train2_emb = rand_forest.apply(X_train2)\n",
    "X_test_emb = rand_forest.apply(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_train2_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Комбо модель\n",
    "model1 = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "                  ('logreg', LogisticRegression())])\n",
    "model1.fit(X_train2_emb, y_train2)\n",
    "\n",
    "# Простой лес\n",
    "model2 = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Простая лог регрессия\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate([model1, model2, model3]):\n",
    "    try:\n",
    "        y_hat = model.predict_proba(X_test_emb)[:, 1]\n",
    "    except:\n",
    "        y_hat = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_hat)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='model%d' % (i+1))\n",
    "    \n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "476px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
